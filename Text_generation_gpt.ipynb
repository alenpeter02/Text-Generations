{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import unicodedata \n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments\n",
    "import torch\n",
    "import math\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cea9e4",
   "metadata": {},
   "source": [
    "# Dataset Source and Filtering\n",
    "\n",
    "## 1. Source\n",
    "The books were taken from **Project Gutenberg**, a free website with public-domain books.  \n",
    "We collected book titles, authors, and links from the main search page sorted by downloads.\n",
    "\n",
    "\n",
    "## 2. Filtering Steps\n",
    "\n",
    "### 2.1 Remove Poetry\n",
    "Poetry books were removed because poems do not follow normal sentence structure.  \n",
    "A book was skipped if its page showed tags like **“poetry”** or **“poem.”**\n",
    "\n",
    "### 2.2 Keep Only Plain Text Files\n",
    "For each book, we looked for the **“Plain Text UTF-8”** download link.  \n",
    "If a book did not have this link, it was ignored.\n",
    "\n",
    "### 2.3 Filter by Size\n",
    "We checked the file size of each text file using a `HEAD` request.  \n",
    "Then we sorted the books from smallest to largest and picked the **10 smallest** ones.\n",
    "\n",
    "\n",
    "## 3. Final Dataset\n",
    "The final dataset contains:\n",
    "- Books from Project Gutenberg  \n",
    "- Only non-poetry books  \n",
    "- Only books with a plain text file  \n",
    "- The smallest 10 books after filtering  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dee8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfaa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-poetry books: 25\n",
      "\n",
      "Processing: The Legend of Sleepy Hollow\n",
      "Downloaded: downloads/The Legend of Sleepy Hollow.txt\n",
      "\n",
      "Processing: The Strange Case of Dr. Jekyll and Mr. Hyde\n",
      "Downloaded: downloads/The Strange Case of Dr Jekyll and Mr Hyde.txt\n",
      "\n",
      "Processing: Romeo and Juliet\n",
      "Downloaded: downloads/Romeo and Juliet.txt\n",
      "\n",
      "Processing: Alice's Adventures in Wonderland\n",
      "Downloaded: downloads/Alices Adventures in Wonderland.txt\n",
      "\n",
      "Processing: Beowulf: An Anglo-Saxon Epic Poem\n",
      "Downloaded: downloads/Beowulf An Anglo-Saxon Epic Poem.txt\n",
      "\n",
      "Processing: A Room with a View\n",
      "Downloaded: downloads/A Room with a View.txt\n",
      "\n",
      "Processing: Cranford\n",
      "Downloaded: downloads/Cranford.txt\n",
      "\n",
      "Processing: The Blue Castle: a novel\n",
      "Downloaded: downloads/The Blue Castle a novel.txt\n",
      "\n",
      "Processing: The King in Yellow\n",
      "Downloaded: downloads/The King in Yellow.txt\n",
      "\n",
      "Processing: Frankenstein; Or, The Modern Prometheus\n",
      "Downloaded: downloads/Frankenstein Or The Modern Prometheus.txt\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://www.gutenberg.org\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "\n",
    "def get_books_from_url(url):\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    books = []\n",
    "\n",
    "    for li in soup.select(\"li.booklink\"):\n",
    "        a = li.find(\"a\")\n",
    "        if not a:\n",
    "            continue\n",
    "\n",
    "        book_url = BASE_URL + a[\"href\"]\n",
    "        title = li.select_one(\".title\").get_text(strip=True)\n",
    "        author = li.select_one(\".subtitle\")\n",
    "        author = author.get_text(strip=True) if author else \"Unknown\"\n",
    "\n",
    "        books.append({\"title\": title, \"author\": author, \"url\": book_url})\n",
    "    return books\n",
    "\n",
    "def is_poem(book_url):\n",
    "    resp = requests.get(book_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    subjects = [s.get_text(strip=True) for s in soup.select(\"td.property_value\")]\n",
    "\n",
    "    for s in subjects:\n",
    "        if \"poetry\" in s.lower() or \"poem\" in s.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_text_download_link(book_url):\n",
    "    resp = requests.get(book_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for a in soup.select(\"a.link\"):\n",
    "        if \"Plain Text UTF-8\" in a.text:\n",
    "            href = a[\"href\"]\n",
    "            if href.startswith(\"//\"):\n",
    "                return \"https:\" + href\n",
    "            if href.startswith(\"/\"):\n",
    "                return BASE_URL + href\n",
    "            return href\n",
    "    return None\n",
    "\n",
    "def get_file_size(url):\n",
    "    try:\n",
    "        resp = requests.head(url, headers=HEADERS, allow_redirects=True)\n",
    "        size = resp.headers.get(\"Content-Length\")\n",
    "        return int(size) if size else float(\"inf\")\n",
    "    except:\n",
    "        return float(\"inf\")\n",
    "\n",
    "def download_book(title, txt_url):\n",
    "    os.makedirs(\"downloads\", exist_ok=True)\n",
    "    safe_title = \"\".join(c for c in title if c.isalnum() or c in \" _-\")\n",
    "    filepath = f\"downloads/{safe_title}.txt\"\n",
    "    resp = requests.get(txt_url, headers=HEADERS)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "    print(\"Downloaded:\", filepath)\n",
    "\n",
    "# Main Pipeline \n",
    "\n",
    "url = \"https://www.gutenberg.org/ebooks/search/?sort_order=downloads\"\n",
    "all_books = get_books_from_url(url)\n",
    "\n",
    "# Filter non-poetry books\n",
    "non_poem_books = [b for b in all_books if not is_poem(b[\"url\"])]\n",
    "print(\"Non-poetry books:\", len(non_poem_books))\n",
    "\n",
    "# Add size info\n",
    "for book in non_poem_books:\n",
    "    txt_link = get_text_download_link(book[\"url\"])\n",
    "    if txt_link:\n",
    "        book[\"txt_link\"] = txt_link\n",
    "        book[\"size\"] = get_file_size(txt_link)\n",
    "    else:\n",
    "        book[\"txt_link\"] = None\n",
    "        book[\"size\"] = float(\"inf\")\n",
    "\n",
    "# Sort by size (ascending) and select first 10\n",
    "books_to_download = sorted([b for b in non_poem_books if b[\"txt_link\"]], key=lambda x: x[\"size\"])[:10]\n",
    "\n",
    "# Download\n",
    "for book in books_to_download:\n",
    "    print(\"\\nProcessing:\", book[\"title\"])\n",
    "    download_book(book[\"title\"], book[\"txt_link\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc64258",
   "metadata": {},
   "source": [
    "## **Text Preprocessing**\n",
    "\n",
    "The preprocessing pipeline is designed to clean, normalize, and structure raw text data \n",
    "for analysis or modeling. The workflow systematically removes noise while retaining meaningful content.\n",
    "\n",
    "**1. Unicode Normalization**\n",
    "- Text is normalized using NFKC to ensure consistent encoding for characters with multiple representations.\n",
    "\n",
    "**2. Line Ending Standardization**\n",
    "- Windows-style (\\r\\n) and old Mac-style (\\r) line endings are converted to Unix-style (\\n).\n",
    "\n",
    "**3. HTML Tag Removal**\n",
    "- All HTML tags are removed using regular expressions to prevent markup from interfering with the text.\n",
    "\n",
    "**4. Non-text Character Cleaning**\n",
    "- Characters except alphanumeric, common punctuation (. , ? ! ; : () ' -), and newlines are removed.\n",
    "\n",
    "**5. Space Normalization**\n",
    "- Consecutive spaces are collapsed into a single space while preserving newlines.\n",
    "- Leading and trailing whitespace is stripped.\n",
    "\n",
    "**6. Chapter and Section Title Removal**\n",
    "- Chapter, book, and part headings are removed using regex patterns.\n",
    "- Short all-caps lines (≤6 words) are excluded.\n",
    "- SpaCy POS tagging is applied to remove short lines consisting mostly of NOUN, PROPN, or NUM.\n",
    "\n",
    "**7. Paragraph Normalization**\n",
    "- Long paragraphs are tokenized using spaCy.\n",
    "- Text is normalized and processed in chunks to prevent memory issues.\n",
    "\n",
    "**8. Line Merging**\n",
    "- Lines within paragraphs are merged into single lines while preserving paragraph separation.\n",
    "\n",
    "**9. Punctuation Correction**\n",
    "- Extra spaces before punctuation are removed for cleaner sentence boundaries.\n",
    "\n",
    "**10. Gutenberg-specific Cleaning**\n",
    "- Headers and footers (e.g., \"*** START OF THE PROJECT GUTENBERG EBOOK ***\") are removed.\n",
    "- Trailing occurrences of \"THE END\" are deleted.\n",
    "\n",
    "**11. File Handling**\n",
    "- Text is read in small chunks for efficient handling of large files.\n",
    "- Cleaned output is saved to a designated directory with robust error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f36198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chunk(text):\n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Convert Windows CRLF to UNIX newlines\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Remove HTML tags if any\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove non-text garbage but KEEP newlines\n",
    "    text = re.sub(r\"[^A-Za-z0-9 .,?!;:()'\\n\\-]+\", \" \", text)\n",
    "\n",
    "    # Collapse many spaces but KEEP newlines\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
    "\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a2f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Regex patterns for chapter headings\n",
    "chapter_patterns = [\n",
    "    r\"^\\s*chapter\\s+[0-9ivxlcdm]+\\s*$\",            # CHAPTER 1, CHAPTER I\n",
    "    r\"^\\s*chapter\\s*[:.\\- ]\\s*[a-zA-Z0-9]+.*$\",    # Chapter: One\n",
    "    r\"^\\s*[0-9ivxlcdm]+\\s*\\.\\s*.*$\",               # 1. The Incident, I. The Beginning\n",
    "    r\"^\\s*book\\s+[0-9ivxlcdm]+\\s*$\",               # BOOK I\n",
    "    r\"^\\s*part\\s+[0-9ivxlcdm]+\\s*$\",               # PART II\n",
    "]\n",
    "\n",
    "chapter_regex = re.compile(\"|\".join(chapter_patterns), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def remove_chapter_titles(text):\n",
    "    cleaned_lines = []\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Skip empty lines\n",
    "        if not stripped:\n",
    "            cleaned_lines.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # Regex chapter detection\n",
    "        if chapter_regex.match(stripped):\n",
    "            continue\n",
    "\n",
    "        # ALL-CAPS titles (likely chapter or book title)\n",
    "        if stripped.isupper() and len(stripped.split()) <= 6:\n",
    "            continue\n",
    "\n",
    "        # spaCy detection: lines with only nouns/proper nouns \n",
    "        doc = nlp(stripped)\n",
    "        pos_tags = {token.pos_ for token in doc}\n",
    "\n",
    "        # Most chapter titles are noun-only lines\n",
    "        if pos_tags.issubset({\"NOUN\", \"PROPN\", \"NUM\"}):\n",
    "            if len(doc) <= 8:   # Avoid deleting sentences\n",
    "                continue\n",
    "\n",
    "        # Keep the line\n",
    "        cleaned_lines.append(stripped)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55c384d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chapter_titles_spacy(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            cleaned_lines.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # Remove trailing punctuation\n",
    "        stripped_clean = stripped.rstrip(\".:;\")\n",
    "\n",
    "        # Regex detection\n",
    "        if chapter_regex.match(stripped_clean):\n",
    "            continue\n",
    "\n",
    "        # Short lines starting with \"Part\" / \"Chapter\" / \"Book\"\n",
    "        if re.match(r\"^(Part|Chapter|Book)\\s+[IVXLCDM0-9a-z]+\\.?$\", stripped_clean, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # ALL CAPS short titles\n",
    "        if stripped_clean.isupper() and len(stripped_clean.split()) <= 6:\n",
    "            continue\n",
    "\n",
    "        # spaCy POS check\n",
    "        doc = nlp(stripped_clean)\n",
    "        pos_tags = {token.pos_ for token in doc}\n",
    "        if pos_tags.issubset({\"NOUN\", \"PROPN\", \"NUM\"}) and len(doc) <= 8:\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(stripped)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines).strip()\n",
    "\n",
    "def spacy_normalize(text, chunk_size=100_000):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    normalized_paragraphs = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        # Process paragraph in smaller chunks if too long\n",
    "        para_tokens = []\n",
    "        for i in range(0, len(para), chunk_size):\n",
    "            chunk = para[i:i+chunk_size]\n",
    "            doc = nlp(chunk)\n",
    "            para_tokens.extend([tok.text for tok in doc if not tok.is_space])\n",
    "\n",
    "        # collapse only double spaces, preserve newlines\n",
    "        normalized = re.sub(r\" {2,}\", \" \", \" \".join(para_tokens))\n",
    "        normalized_paragraphs.append(normalized)\n",
    "        \n",
    "    # Join paragraphs with double newline\n",
    "    return \"\\n\\n\".join(normalized_paragraphs).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b5526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_paragraph_lines(text):\n",
    "    \"\"\"\n",
    "    Merge lines inside a paragraph, keep paragraphs separated by double newlines.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    merged_paragraphs = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        # Merge single line breaks into space\n",
    "        para = re.sub(r\"\\n+\", \" \", para)\n",
    "        merged_paragraphs.append(para)\n",
    "\n",
    "    return \"\\n\\n\".join(merged_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66c2afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORY CONFIG\n",
    "RAW_DIR = r\"downloads\"   # \n",
    "CLEAN_DIR = r\"clean_books\"  #\n",
    "\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "554c627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Main preprocessing pipeline\n",
    "def preprocess_file(file_name):\n",
    "    raw_path = os.path.join(RAW_DIR, file_name)\n",
    "    clean_path = os.path.join(CLEAN_DIR, file_name)\n",
    "\n",
    "    try:\n",
    "        # READ RAW FILE\n",
    "        chunks = []\n",
    "        with open(raw_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin:\n",
    "            while True:\n",
    "                part = fin.read(8192)\n",
    "                if not part:\n",
    "                    break\n",
    "                chunks.append(part)\n",
    "\n",
    "        text = \"\".join(chunks)\n",
    "\n",
    "        # REMOVE GUTENBERG HEADER\n",
    "        start_match = re.search(r\"\\*\\*\\* START OF.*?\\*\\*\\*\", text, re.IGNORECASE)\n",
    "        if start_match:\n",
    "            text = text[start_match.end():]\n",
    "\n",
    "        # REMOVE GUTENBERG FOOTER\n",
    "        end_match = re.search(r\"\\*\\*\\* END OF.*?\\*\\*\\*\", text, re.IGNORECASE)\n",
    "        if end_match:\n",
    "            text = text[:end_match.start()]\n",
    "\n",
    "        text = re.sub(\n",
    "            r\"END OF THE PROJECT GUTENBERG EBOOK.*\",\n",
    "            \"\",\n",
    "            text,\n",
    "            flags=re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "\n",
    "        text = re.sub(\n",
    "            r\"\\bTHE END\\b[\\s\\n]*$\",\n",
    "            \"\",\n",
    "            text,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        # CLEAN MAIN CONTENT\n",
    "        cleaned = clean_chunk(text)\n",
    "\n",
    "        # REMOVE CHAPTER HEADERS\n",
    "        cleaned = remove_chapter_titles(cleaned)\n",
    "\n",
    "        # LIGHT SPACY NORMALIZATION\n",
    "        cleaned = spacy_normalize(cleaned)\n",
    "\n",
    "        # Merge lines within paragraphs\n",
    "        cleaned = merge_paragraph_lines(cleaned)\n",
    "\n",
    "        # Fix spaces before punctuation\n",
    "        cleaned = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", cleaned)\n",
    "\n",
    "        # SAVE CLEANED\n",
    "        with open(clean_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(cleaned.strip() + \"\\n\")\n",
    "\n",
    "        return {\"file_name\": file_name, \"text\": cleaned}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to clean {file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50f3348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning books: 100%|██████████| 10/10 [06:44<00:00, 40.44s/book]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning all books.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [f for f in os.listdir(RAW_DIR) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "for file_name in tqdm(files, desc=\"Cleaning books\", unit=\"book\"):\n",
    "    preprocess_file(file_name)\n",
    "\n",
    "print(\"Finished cleaning all books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52967017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 10 files into merged_books.txt with explicit book boundaries.\n"
     ]
    }
   ],
   "source": [
    "# Directory containing cleaned books\n",
    "CLEAN_DIR = \"clean_books\"\n",
    "OUTPUT_FILE = \"merged_books.txt\"\n",
    "\n",
    "# Special token to mark the end of each book\n",
    "BOOK_END_TOKEN = \"<|BOOK_END|>\"\n",
    "\n",
    "# Get all text files in the directory, sorted alphabetically\n",
    "text_files = sorted([f for f in os.listdir(CLEAN_DIR) if f.endswith(\".txt\")])\n",
    "\n",
    "# Open the output file in write mode (incremental write for memory efficiency)\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for file_name in text_files:\n",
    "        file_path = os.path.join(CLEAN_DIR, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            book_text = infile.read().strip()\n",
    "            # Write book text followed by special token only\n",
    "            outfile.write(book_text + f\"\\n{BOOK_END_TOKEN}\\n\")\n",
    "\n",
    "print(f\"Merged {len(text_files)} files into {OUTPUT_FILE} with explicit book boundaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57282152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MERGED_FILE = \"merged_books.txt\"  # Merged books file    \n",
    "SPECIAL_TOKENS = [\"<|BOOK_END|>\"]  # Special token marking book end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5db75",
   "metadata": {},
   "source": [
    "# Model Training Summary\n",
    "\n",
    "## Model\n",
    "The model used for training is **DistilGPT-2**, a smaller and faster version of GPT-2.  \n",
    "After adding the special token, the model’s vocabulary size was updated to match the tokenizer.\n",
    "\n",
    "\n",
    "## Training Setup\n",
    "Training was done with the following settings:\n",
    "\n",
    "- Batch size: 1  \n",
    "- Gradient accumulation: 8  \n",
    "- Learning rate: 5e-5  \n",
    "- Epochs: 2  \n",
    "- FP16 enabled only if a GPU was available  \n",
    "- Checkpoints saved every 500 steps  \n",
    "- Logs written every 100 steps  \n",
    "\n",
    "Due to limited hardware, training ran slowly and completed fewer updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641205 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and add special tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\n",
    "SPECIAL_TOKENS = [\"<|BOOK_END|>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "\n",
    "# Read merged text\n",
    "with open(\"merged_books.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize AFTER adding special tokens\n",
    "tokens = tokenizer(text)[\"input_ids\"]\n",
    "\n",
    "# Create chunk generator\n",
    "BLOCK_SIZE = 512\n",
    "OVERLAP = 64\n",
    "\n",
    "def chunk_generator(tokens, block_size=BLOCK_SIZE, overlap=OVERLAP):\n",
    "    step = block_size - overlap\n",
    "    for start in range(0, len(tokens), step):\n",
    "        end = start + block_size\n",
    "        chunk = tokens[start:end]\n",
    "        if len(chunk) < block_size:\n",
    "            break  # drop incomplete chunks\n",
    "        yield {\"input_ids\": chunk, \"labels\": chunk}\n",
    "\n",
    "# Build dataset\n",
    "dataset = Dataset.from_generator(lambda: chunk_generator(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and resize embeddings AFTER adding tokens\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a5f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 2:59:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.969300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=358, training_loss=4.0568871098523696, metrics={'train_runtime': 10805.7185, 'train_samples_per_second': 0.265, 'train_steps_per_second': 0.033, 'total_flos': 373915650097152.0, 'train_loss': 4.0568871098523696, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt_books_model\",\n",
    "    per_device_train_batch_size=1,  \n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),  # only if GPU\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a56921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ['added_tokens.json', 'checkpoint-358', 'config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./gpt_books_model\"\n",
    "\n",
    "# Create folder if missing\n",
    "import os\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"Saved:\", os.listdir(save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ace496d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"./gpt_books_model_trained\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23bf3478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the great heroes of this world had been, in fact more than twenty years ago. They were not to be expected; they are very worthy and noble creatures for many reasons: The greatest hero has ever lived with his or her companions! Their love was so precious that it seemed only if he would have made an honest deal by saying what mattered most about us today when we began our lives?\n",
      "We must remember these truths no longer as I know them at present but rather perhaps even through some kind sort de rigeur which will give him strength under all circumstances - one may think otherwise\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./gpt_books_model_trained\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "# Add attention mask explicitly (fixes the warning)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=120,\n",
    "    temperature=0.8,\n",
    "    top_p=0.92,\n",
    "    top_k=40,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f1139f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.1993677362836\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, labels=input_ids).loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "print(compute_perplexity(\"Once upon a time...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c365825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.4111336169005197\n"
     ]
    }
   ],
   "source": [
    "reference = [\"Once upon a time there was a king\".split()]\n",
    "hypothesis = \"Once upon a time a king lived happily\".split()\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth)\n",
    "\n",
    "print(\"BLEU:\", bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e645b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39bbd968",
   "metadata": {},
   "source": [
    "## **Model Performance Report**\n",
    "\n",
    "The model shows average results mainly because the training setup and hardware were limited.\n",
    "\n",
    "### Reasons:\n",
    "- **Weak hardware:** Training was slow (very few steps per second). The model could not learn enough patterns from the data.\n",
    "- **Only 2 epochs:** For GPT-2 models, this is too small. Good results usually need many more training steps.\n",
    "- **Small model (DistilGPT-2):** This version has fewer layers and cannot learn deep patterns like the full GPT-2.\n",
    "- **Small batch size:** Batch size = 1 makes training noisy and less stable.\n",
    "\n",
    "### Metric explanation:\n",
    "- **Perplexity ≈ 478:** The model can produce readable text, but it still struggles to predict the next word accurately.\n",
    "- **BLEU ≈ 0.41:** The generated text matches the reference text only a little. This is common when the model is trained for a short time.\n",
    "\n",
    "\n",
    "## **Model Performs in Text Generation**\n",
    "### Strengths:\n",
    "- Produces clear and grammatically correct sentences.\n",
    "- Follows the general writing style of books.\n",
    "- Works well for short text (1–3 sentences).\n",
    "\n",
    "### Weaknesses:\n",
    "- Loses meaning in longer paragraphs.\n",
    "- Cannot keep story details consistent.\n",
    "\n",
    "### Overall:\n",
    "The model works, but it is not fully trained because of limited compute and short training time. With more epochs and better hardware, its text quality and metrics would improve a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txt_env (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
